{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "serious-budget",
   "metadata": {},
   "source": [
    "# Histogram fits with `pyhf`\n",
    "\n",
    "Often we don't have a clear way to parametrize our fit templates, so we need to resort to MC simulations and use histograms as templates that we fit to data in the same bins.\n",
    "\n",
    "We are going to use the [`pyhf`](https://github.com/scikit-hep/pyhf) package for these fits. The documentation can be found at https://pyhf.readthedocs.io/.\n",
    "\n",
    "It can be installed with pip, e.g.\n",
    "\n",
    "`pip install --user pyhf`\n",
    "\n",
    "In addition we are using the `mplhep` package in this notebook to make histogram plotting more convenient and `iminuit` to extract uncertainties on fit parameters.\n",
    "\n",
    "`pip install --user mplhep iminuit`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "trying-owner",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyhf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "continued-management",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import mplhep as hep"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "laden-thing",
   "metadata": {},
   "source": [
    "Let's create 2 artificial histograms with 10 bins (having 11 bin boundaries). You could imagine these as two different background processes for which we have MC simulations on which we ran some event selection and created histograms for. For now, let's assume that the shape of these distributions comes out correctly and we only need to fit the normalization (for both templates independently) to data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accepted-conflict",
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = np.arange(11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "threaded-watts",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist1 = np.array([1.5, 3., 6., 7.5, 6.3, 6.6, 6., 4.5, 3. , 1.5])\n",
    "hist2 = np.array([3. , 6., 9., 12., 15., 9. , 6., 3. , 0.3, 0.15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "contemporary-limit",
   "metadata": {},
   "outputs": [],
   "source": [
    "hep.histplot([hist1, hist2], bins)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "single-zealand",
   "metadata": {},
   "source": [
    "We want to stack them since we think the sum of both should give us the expected data yield"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "modern-shame",
   "metadata": {},
   "outputs": [],
   "source": [
    "hep.histplot([hist1, hist2], bins, stack=True, histtype=\"fill\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "orange-korean",
   "metadata": {},
   "source": [
    "Now, let's assume we observed the following data counts in each bin:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "temporal-inclusion",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.array([ 4, 17, 26, 23, 34, 23, 21,  7,  8,  4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "racial-blind",
   "metadata": {},
   "outputs": [],
   "source": [
    "hep.histplot([hist1, hist2], bins, stack=True, histtype=\"fill\")\n",
    "hep.histplot(data, bins, histtype=\"errorbar\", color=\"black\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nearby-darkness",
   "metadata": {},
   "source": [
    "Oftentimes, one plots errorbars, indicating 1 $\\sigma$ confidence intervals on poisson distributed event counts to have some visualization on the expected spread."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hearing-paradise",
   "metadata": {},
   "outputs": [],
   "source": [
    "hep.histplot([hist1, hist2], bins, stack=True, histtype=\"fill\")\n",
    "hep.histplot(data, bins, histtype=\"errorbar\", color=\"black\", w2=data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "muslim-grounds",
   "metadata": {},
   "source": [
    "## One template fits it all\n",
    "\n",
    "`pyhf` does fits using the Maximum-Likelihood method and uses the HistFactory ([CERN-OPEN-2012-016](https://cds.cern.ch/record/1456844)) template. In the simplemost case the pdf (probability density function) is just a product of poisson counts in each bin:\n",
    "\n",
    "$$p(n|\\lambda) = \\prod_{\\mathrm{bin}\\, b} \\mathrm{Pois}(n_b | \\lambda_b)$$\n",
    "\n",
    "where $\\mathrm{Pois}(n_b | \\lambda_b)$ is the Poisson distribution for $\\lambda_b$ expected and $n_b$ observed counts. In our case $\\lambda_b$ would be given by\n",
    "\n",
    "$$\\lambda_b = \\mu_1 b_1 + \\mu_2 b_2$$\n",
    "\n",
    "where $b_1$ and $b_2$ are the expected counts from our 2 histograms and $\\mu_1$ and $\\mu_2$ are the normalization factors we want to fit. This pdf will define the Likelihood function that is later maximized to give the best fitting parameter values.\n",
    "\n",
    "The general template is more complicated, allowing for constraint terms and separation into arbitrary channels - we will come back to that later.\n",
    "\n",
    "Models in `pyhf` are defined with a json-like specification. In our case we can define the model with the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decent-projector",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = [\n",
    "    {\n",
    "        \"name\": \"sample1\",\n",
    "        \"data\": list(hist1),\n",
    "        \"modifiers\": [\n",
    "            {\"name\": \"mu1\", \"type\": \"normfactor\", \"data\" : None}\n",
    "        ],\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"sample2\",\n",
    "        \"data\": list(hist2),\n",
    "        \"modifiers\": [\n",
    "            {\"name\": \"mu2\", \"type\": \"normfactor\", \"data\" : None}\n",
    "        ],\n",
    "    },\n",
    "]\n",
    "spec = {\"channels\" : [{\"name\" : \"singlechannel\", \"samples\" : samples}]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stone-young",
   "metadata": {},
   "outputs": [],
   "source": [
    "# info: the `poi_name=None` is nescessary here since we don't want to do a hypothesis test\n",
    "model = pyhf.Model(spec, poi_name=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brief-inclusion",
   "metadata": {},
   "source": [
    "We will now run a *maximum likelihood fit* that gives us the parameters that maximize the likelihood, the *maximum likelihood estimates* (mle)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sharing-carry",
   "metadata": {},
   "outputs": [],
   "source": [
    "mu1, mu2 = pyhf.infer.mle.fit(data, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "consistent-hello",
   "metadata": {},
   "outputs": [],
   "source": [
    "mu1, mu2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "official-desire",
   "metadata": {},
   "source": [
    "We did not have to specify initial parameter values or bounds. For normalization factors the initial parameters are by default `1` and the bounds (fit range) is `[0, 10]`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reserved-monitor",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.suggested_init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "technical-future",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.suggested_bounds()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cardiovascular-trail",
   "metadata": {},
   "source": [
    "Let's look at the fitted templates, together with the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lonely-appraisal",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hep.histplot([mu1 * hist1, mu2 * hist2], bins, stack=True, histtype=\"fill\")\n",
    "hep.histplot([mu1 * hist1, mu2 * hist2], bins, stack=True, histtype=\"fill\")\n",
    "hep.histplot(data, bins, histtype=\"errorbar\", color=\"black\", w2=data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "perfect-detection",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    <b>Exercise:</b> Repeat the fit, but use the same normalization factor for both samples. <i>Hint:</i> parameters in the pyhf specification are just identified by their name and the same name can occur in multiple places in the spec.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "strange-pennsylvania",
   "metadata": {},
   "source": [
    "## Uncertainties on fit parameters and the \"post-fit\" plot\n",
    "\n",
    "Often, we are also interested in the uncertainties and correlations between fit parameters. We can use `iminuit` as a fitting backend for `pyhf` to extract them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "smooth-squad",
   "metadata": {},
   "outputs": [],
   "source": [
    "pyhf.set_backend('numpy', 'minuit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "primary-track",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters, correlations = pyhf.infer.mle.fit(data, model, return_uncertainties=True, return_correlations=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "smooth-dutch",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suburban-romance",
   "metadata": {},
   "outputs": [],
   "source": [
    "correlations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "specialized-flavor",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    <b>Questions</b>: Why are the fit parameters correlated? Why is the correlation coefficient negative?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "protected-colorado",
   "metadata": {},
   "source": [
    "We can visualize the impact of these uncertainties on our fit templates using [linear error propagation](https://en.wikipedia.org/wiki/Propagation_of_uncertainty).\n",
    "\n",
    "In this simple case, let's calculate this manually:\n",
    "\n",
    "$$\\sigma_{\\lambda_b}^2 = \\left(\\frac{\\partial \\lambda_b}{\\partial \\mu_1}\\sigma_{\\mu_1}\\right)^2 + \\left(\\frac{\\partial \\lambda_b}{\\partial \\mu_2}\\sigma_{\\mu_2}\\right)^2 + 2 \\frac{\\partial \\lambda_b}{\\partial \\mu_1}\\frac{\\partial \\lambda_b}{\\partial \\mu_2}\\sigma_{\\mu_1}\\sigma_{\\mu_2}\\rho_{12} = \\left(b_1\\sigma_{\\mu_1}\\right)^2 + \\left(b_2\\sigma_{\\mu_2}\\right)^2 + 2 b_1 b_2 \\sigma_{b_1}\\sigma_{b_2}\\rho_{12}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "resistant-harvey",
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma1, sigma2 = parameters[:, 1]\n",
    "sigma1, sigma2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unknown-drain",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_error = np.sqrt(\n",
    "    (hist1 * sigma1) ** 2 + (hist2 * sigma2) ** 2 + 2 * hist1 * hist2 * sigma1 * sigma2 * correlations[0][1]\n",
    ")\n",
    "hist_error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "environmental-walker",
   "metadata": {},
   "source": [
    "For more generic templates/functions you can do that automatically. Either use the [`uncertainties`](https://pythonhosted.org/uncertainties/) package (for functions described by simple formulas) or calculate it numerically by varying each parameter up and down and using half of the resulting interval as a replacement for $\\frac{\\partial f}{\\partial x_i}\\sigma_{x_i}$.\n",
    "\n",
    "A generic function for using this method with `pyhf` models is currently provided by the [`cabinetry`](https://github.com/alexander-held/cabinetry) package from Alexander Held with the function [`cabinetry.model_utils.calculate_stdev`](https://github.com/alexander-held/cabinetry/blob/c8668005e899556675b5e646e127908849bfe597/src/cabinetry/model_utils.py#L176)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "major-indonesian",
   "metadata": {},
   "outputs": [],
   "source": [
    "def errorband(bins, hist, hist_error, ax=None):\n",
    "    ax = ax or plt.gca()\n",
    "    n = hist\n",
    "    s = hist_error\n",
    "\n",
    "    def extend(x):\n",
    "        return np.append(x, x[-1])\n",
    "\n",
    "    ax.fill_between(\n",
    "        bins,\n",
    "        extend(n - s),\n",
    "        extend(n + s),\n",
    "        step=\"post\",\n",
    "        color=\"black\",\n",
    "        alpha=0.3,\n",
    "        linewidth=0\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "digital-retail",
   "metadata": {},
   "outputs": [],
   "source": [
    "hep.histplot([mu1 * hist1, mu2 * hist2], bins, stack=True, histtype=\"fill\")\n",
    "hep.histplot(data, bins, histtype=\"errorbar\", color=\"black\", w2=data)\n",
    "errorband(bins, mu1 * hist1 + mu2 * hist2, hist_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rational-edition",
   "metadata": {},
   "source": [
    "This errorband is often referred to as \"post-fit\" uncertainties."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "theoretical-breach",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    <b>Note:</b> Since the same normalization factor is used across all bins, the uncertainty on the template depends on all data points. Therefore the interval is smaller than the expected fluctuation in each individual bin.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "improved-nothing",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    <b>Question:</b> What would happen if you fit an independent template per histogram bin? What would the uncertainty look like?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "worst-beast",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    <b>Exercise [hard]:</b> Try it out! Hint: One way to do this is by defining a sample for each bin (with an individual normalization factor for each bin) and setting all other bins to 0.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "descending-feedback",
   "metadata": {},
   "source": [
    "# Advanced:  Uncertainties on the histogram templates"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
