{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cubic-scoop",
   "metadata": {},
   "source": [
    "# Histogram fits with `pyhf`\n",
    "\n",
    "Often we don't have a clear way to parametrize our fit templates, so we need to resort to MC simulations and use histograms as templates that we fit to data in the same bins.\n",
    "\n",
    "We are going to use the [`pyhf`](https://github.com/scikit-hep/pyhf) package for these fits. The documentation can be found at https://pyhf.readthedocs.io/.\n",
    "\n",
    "It can be installed with pip, e.g.\n",
    "\n",
    "`pip install --user pyhf`\n",
    "\n",
    "In addition we are using the `mplhep` package in this notebook to make histogram plotting more convenient and `iminuit` to extract uncertainties on fit parameters.\n",
    "\n",
    "`pip install --user mplhep iminuit`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "literary-diagnosis",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyhf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "typical-ticket",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import mplhep as hep"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "executive-employment",
   "metadata": {},
   "source": [
    "Let's create 2 artificial histograms with 10 bins (having 11 bin boundaries). You could imagine these as two different background processes for which we have MC simulations on which we ran some event selection and created histograms for. For now, let's assume that the shape of these distributions comes out correctly and we only need to fit the normalization (for both templates independently) to data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "contained-toner",
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = np.arange(11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bright-toolbox",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist1 = 3 * np.array([0.5, 1, 2, 2.5, 2.1, 2.2, 2, 1.5, 1, 0.5])\n",
    "hist2 = 3 * np.array([1, 2, 3, 4, 5, 3, 2, 1, 0.1, 0.05])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alien-business",
   "metadata": {},
   "outputs": [],
   "source": [
    "hep.histplot([hist1, hist2], bins)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "received-mayor",
   "metadata": {},
   "source": [
    "We want to stack them since we think the sum of both should give us the expected data yield"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "departmental-layout",
   "metadata": {},
   "outputs": [],
   "source": [
    "hep.histplot([hist1, hist2], bins, stack=True, histtype=\"fill\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dental-forest",
   "metadata": {},
   "source": [
    "Now, let's assume we observed the following data counts in each bin:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "through-initial",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.array([ 4, 17, 26, 23, 34, 23, 21,  7,  8,  4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "strong-swift",
   "metadata": {},
   "outputs": [],
   "source": [
    "hep.histplot([hist1, hist2], bins, stack=True, histtype=\"fill\")\n",
    "hep.histplot(data, bins, histtype=\"errorbar\", color=\"black\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "widespread-finance",
   "metadata": {},
   "source": [
    "Oftentimes, one plots errorbars, indicating 1 $\\sigma$ confidence intervals on poisson distributed event counts to have some visualization on the expected spread."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "visible-correlation",
   "metadata": {},
   "outputs": [],
   "source": [
    "hep.histplot([hist1, hist2], bins, stack=True, histtype=\"fill\")\n",
    "hep.histplot(data, bins, histtype=\"errorbar\", color=\"black\", w2=data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "published-belgium",
   "metadata": {},
   "source": [
    "## One template fits it all\n",
    "\n",
    "`pyhf` does fits using the Maximum-Likelihood method and uses the HistFactory ([CERN-OPEN-2012-016](https://cds.cern.ch/record/1456844)) template. In the simplemost case the pdf (probability density function) is just a product of poisson counts in each bin:\n",
    "\n",
    "$$p(n|\\lambda) = \\prod_{\\mathrm{bin}\\, b} \\mathrm{Pois}(n_b | \\lambda_b)$$\n",
    "\n",
    "where $\\mathrm{Pois}(n_b | \\lambda_b)$ is the Poisson distribution for $\\lambda_b$ expected and $n_b$ observed counts. In our case $\\lambda_b$ would be given by\n",
    "\n",
    "$$\\lambda_b = \\mu_1 b_1 + \\mu_2 b_2$$\n",
    "\n",
    "where $b_1$ and $b_2$ are the expected counts from our 2 histograms and $\\mu_1$ and $\\mu_2$ are the normalization factors we want to fit. This pdf will define the Likelihood function that is later maximized to give the best fitting parameter values.\n",
    "\n",
    "The general template is more complicated, allowing for constraint terms and separation into arbitrary channels, but we will come back to that later.\n",
    "\n",
    "Models in `pyhf` are defined with a json-like specification. In our case we can define the model with the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "individual-termination",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = [\n",
    "    {\n",
    "        \"name\": \"sample1\",\n",
    "        \"data\": list(hist1),\n",
    "        \"modifiers\": [\n",
    "            {\"name\": \"mu1\", \"type\": \"normfactor\", \"data\" : None}\n",
    "        ],\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"sample2\",\n",
    "        \"data\": list(hist2),\n",
    "        \"modifiers\": [\n",
    "            {\"name\": \"mu2\", \"type\": \"normfactor\", \"data\" : None}\n",
    "        ],\n",
    "    },\n",
    "]\n",
    "spec = {\"channels\" : [{\"name\" : \"singlechannel\", \"samples\" : samples}]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vertical-trial",
   "metadata": {},
   "outputs": [],
   "source": [
    "# info: the `poi_name=None` is nescessary here since we don't want to do a hypothesis test\n",
    "model = pyhf.Model(spec, poi_name=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "color-awareness",
   "metadata": {},
   "source": [
    "We will now run a *maximum likelihood fit* that gives us the parameters that maximize the likelihood, the *maximum likelihood estimates* (mle)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "enclosed-laser",
   "metadata": {},
   "outputs": [],
   "source": [
    "mu1, mu2 = pyhf.infer.mle.fit(data, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "committed-disaster",
   "metadata": {},
   "outputs": [],
   "source": [
    "mu1, mu2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "integrated-fourth",
   "metadata": {},
   "source": [
    "We did not have to specify initial parameter values or bounds. For normalization factors the initial parameters are by default `1` and the bounds (fit range) is `[0, 10]`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "authorized-dublin",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.suggested_init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "weekly-macintosh",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.suggested_bounds()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "based-constitutional",
   "metadata": {},
   "source": [
    "Let's look at the fitted templates, together with the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "parliamentary-consortium",
   "metadata": {},
   "outputs": [],
   "source": [
    "hep.histplot([mu1 * hist1, mu2 * hist2], bins, stack=True, histtype=\"fill\")\n",
    "hep.histplot(data, bins, histtype=\"errorbar\", color=\"black\", w2=data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unexpected-rubber",
   "metadata": {},
   "source": [
    "Often, we are also interested in the uncertainties and correlations between fit parameters. We can use `iminuit` as a fitting backend for `pyhf` to extract them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chief-stroke",
   "metadata": {},
   "outputs": [],
   "source": [
    "pyhf.set_backend('numpy', 'minuit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "excess-travel",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters, correlations = pyhf.infer.mle.fit(data, model, return_uncertainties=True, return_correlations=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sought-chinese",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "separated-polls",
   "metadata": {},
   "outputs": [],
   "source": [
    "correlations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "multiple-revision",
   "metadata": {},
   "source": [
    "We can visualize the impact of these uncertainties on our fit templates using [linear error propagation](https://en.wikipedia.org/wiki/Propagation_of_uncertainty).\n",
    "\n",
    "In this simple case, let's calculate this manually:\n",
    "\n",
    "$$\\sigma_{\\lambda_b}^2 = \\left(\\frac{\\partial \\lambda_b}{\\partial \\mu_1}\\sigma_{\\mu_1}\\right)^2 + \\left(\\frac{\\partial \\lambda_b}{\\partial \\mu_2}\\sigma_{\\mu_2}\\right)^2 + 2 \\frac{\\partial \\lambda_b}{\\partial \\mu_1}\\frac{\\partial \\lambda_b}{\\partial \\mu_2}\\sigma_{\\mu_1}\\sigma_{\\mu_2}\\rho_{12} = \\left(b_1\\sigma_{\\mu_1}\\right)^2 + \\left(b_2\\sigma_{\\mu_2}\\right)^2 + 2 b_1 b_2 \\sigma_{b_1}\\sigma_{b_2}\\rho_{12}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "straight-train",
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma1, sigma2 = parameters[:, 1]\n",
    "sigma1, sigma2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "iraqi-folder",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_error = np.sqrt(\n",
    "    (hist1 * sigma1) ** 2 + (hist2 * sigma2) ** 2 + 2 * hist1 * hist2 * sigma1 * sigma2 * correlations[0][1]\n",
    ")\n",
    "hist_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "recorded-accommodation",
   "metadata": {},
   "outputs": [],
   "source": [
    "def errorband(bins, hist, hist_error, ax=None):\n",
    "    ax = ax or plt.gca()\n",
    "    n = hist\n",
    "    s = hist_error\n",
    "\n",
    "    def extend(x):\n",
    "        return np.append(x, x[-1])\n",
    "\n",
    "    ax.fill_between(\n",
    "        bins,\n",
    "        extend(n - s),\n",
    "        extend(n + s),\n",
    "        step=\"post\",\n",
    "        color=\"black\",\n",
    "        alpha=0.3,\n",
    "        linewidth=0\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "massive-passion",
   "metadata": {},
   "outputs": [],
   "source": [
    "hep.histplot([mu1 * hist1, mu2 * hist2], bins, stack=True, histtype=\"fill\")\n",
    "hep.histplot(data, bins, histtype=\"errorbar\", color=\"black\", w2=data)\n",
    "errorband(bins, mu1 * hist1 + mu2 * hist2, hist_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "automotive-rebound",
   "metadata": {},
   "source": [
    "# Advanced:  Uncertainties on the histogram templates"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
