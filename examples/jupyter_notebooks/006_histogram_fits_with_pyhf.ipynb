{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "serious-budget",
   "metadata": {},
   "source": [
    "# Histogram fits with `pyhf`\n",
    "\n",
    "Often we don't have a clear way to parametrize our fit templates, so we need to resort to MC simulations and use histograms as templates that we fit to data in the same bins.\n",
    "\n",
    "We are going to use the [`pyhf`](https://github.com/scikit-hep/pyhf) package for these fits. The documentation can be found at https://pyhf.readthedocs.io/.\n",
    "\n",
    "It can be installed with pip, e.g.\n",
    "\n",
    "`pip install --user pyhf`\n",
    "\n",
    "In addition we are using the `mplhep` package in this notebook to make histogram plotting more convenient and `iminuit` to extract uncertainties on fit parameters.\n",
    "\n",
    "`pip install --user mplhep iminuit`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "trying-owner",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyhf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "continued-management",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import mplhep as hep"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "laden-thing",
   "metadata": {},
   "source": [
    "Let's create 2 artificial histograms with 10 bins (having 11 bin boundaries). You could imagine these as two different background processes for which we have MC simulations on which we ran some event selection and created histograms for. For now, let's assume that the shape of these distributions comes out correctly and we only need to fit the normalization (for both templates independently) to data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accepted-conflict",
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = np.arange(11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "threaded-watts",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist1 = np.array([1.5, 3., 6., 7.5, 6.3, 6.6, 6., 4.5, 3. , 1.5])\n",
    "hist2 = np.array([3. , 6., 9., 12., 15., 9. , 6., 3. , 0.3, 0.15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "contemporary-limit",
   "metadata": {},
   "outputs": [],
   "source": [
    "hep.histplot([hist1, hist2], bins)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "single-zealand",
   "metadata": {},
   "source": [
    "We want to stack them since we think the sum of both should give us the expected data yield"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "modern-shame",
   "metadata": {},
   "outputs": [],
   "source": [
    "hep.histplot([hist1, hist2], bins, stack=True, histtype=\"fill\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "orange-korean",
   "metadata": {},
   "source": [
    "Now, let's assume we observed the following data counts in each bin:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "temporal-inclusion",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.array([ 4, 17, 26, 23, 34, 23, 21,  7,  8,  4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "racial-blind",
   "metadata": {},
   "outputs": [],
   "source": [
    "hep.histplot([hist1, hist2], bins, stack=True, histtype=\"fill\")\n",
    "hep.histplot(data, bins, histtype=\"errorbar\", color=\"black\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nearby-darkness",
   "metadata": {},
   "source": [
    "Oftentimes, one plots errorbars, indicating 1 $\\sigma$ confidence intervals on poisson distributed event counts to have some visualization on the expected spread. This can be done by passing the data as value for the `w2` argument of `histplot`.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "    The parameter <code>w2</code> stands for the \"sum of squares of weights\" $\\sum(w_i^2)$. The square root of this will give us an estimate for the standard deviaton.\n",
    "    This is equal to the event counts for unweighted events (such as the observed data) and in case all values are integer <code>histplot</code> will plot an asymmetric interval based on the assumption of a poisson distribution.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hearing-paradise",
   "metadata": {},
   "outputs": [],
   "source": [
    "hep.histplot([hist1, hist2], bins, stack=True, histtype=\"fill\")\n",
    "hep.histplot(data, bins, histtype=\"errorbar\", color=\"black\", w2=data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "muslim-grounds",
   "metadata": {},
   "source": [
    "## One template fits it all\n",
    "\n",
    "`pyhf` does fits using the Maximum-Likelihood method and uses the HistFactory ([CERN-OPEN-2012-016](https://cds.cern.ch/record/1456844)) template. In the simplemost case the pdf (probability density function) is just a product of poisson counts in each bin:\n",
    "\n",
    "$$p(\\vec n|\\vec\\lambda) = \\prod_{\\mathrm{bin}\\, i} \\mathrm{Pois}(n_i | \\lambda_i)$$\n",
    "\n",
    "where $\\mathrm{Pois}(n_i | \\lambda_i)$ is the Poisson distribution for $\\lambda_i$ expected and $n_i$ observed counts. In our case $\\lambda_i$ would be given by\n",
    "\n",
    "$$\\lambda_i = \\mu_1 b_{i, 1} + \\mu_2 b_{i, 2}$$\n",
    "\n",
    "where $b_{i, 1}$ and $b_{i, 2}$ are the expected counts in bin $i$ of our 2 histograms and $\\mu_1$ and $\\mu_2$ are the normalization factors we want to fit. This pdf will define the Likelihood function that is later maximized to give the best fitting parameter values.\n",
    "\n",
    "The general template is more complicated, allowing for constraint terms and separation into arbitrary channels - we will come back to that later.\n",
    "\n",
    "Models in `pyhf` are defined with a json-like specification. That is, a nested structure of lists and dictionaries. The hierarchy is as follows:\n",
    "\n",
    "* A model can have several **channels**. This can be used to e.g. define different normalization factors for different categories of selection (but they have to be orthogonal)\n",
    "* Each channel can have several **samples**. Each sample comes with a histogram template.\n",
    "* Each sample can have several **modifiers**. These will define the fit parameters. Modifiers can be free normalization factors or constraint parameters (more later)\n",
    "\n",
    "In our case we can define a model with just one channel and two samples which each have a normalization factor (free parameter) as a modifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decent-projector",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = [\n",
    "    {\n",
    "        \"name\": \"sample1\",\n",
    "        \"data\": list(hist1),\n",
    "        \"modifiers\": [\n",
    "            {\"name\": \"mu1\", \"type\": \"normfactor\", \"data\" : None}\n",
    "        ],\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"sample2\",\n",
    "        \"data\": list(hist2),\n",
    "        \"modifiers\": [\n",
    "            {\"name\": \"mu2\", \"type\": \"normfactor\", \"data\" : None}\n",
    "        ],\n",
    "    },\n",
    "]\n",
    "spec = {\"channels\" : [{\"name\" : \"singlechannel\", \"samples\" : samples}]}\n",
    "\n",
    "# info: the `poi_name=None` is nescessary here since we don't want to do a hypothesis test\n",
    "model = pyhf.Model(spec, poi_name=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "    The histogram bin contents need to be specified as a list (not a numpy array), such that we can really dump this into the text based json format.\n",
    "</div>\n",
    "\n",
    "Our specification would look like this as a json string (which can be simply stored in a text file):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dumps(spec)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brief-inclusion",
   "metadata": {},
   "source": [
    "We will now run a *maximum likelihood fit* that gives us the parameters that maximize the likelihood (technically we will minimize the negative log-likelihood), the *maximum likelihood estimates* (mle)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sharing-carry",
   "metadata": {},
   "outputs": [],
   "source": [
    "mu1, mu2 = pyhf.infer.mle.fit(data, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "consistent-hello",
   "metadata": {},
   "outputs": [],
   "source": [
    "mu1, mu2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "official-desire",
   "metadata": {},
   "source": [
    "We did not have to specify initial parameter values or bounds. For normalization factors the initial parameters are by default `1` and the bounds (fit range) is `[0, 10]`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reserved-monitor",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.suggested_init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "technical-future",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.suggested_bounds()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cardiovascular-trail",
   "metadata": {},
   "source": [
    "Let's look at the fitted templates, together with the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lonely-appraisal",
   "metadata": {},
   "outputs": [],
   "source": [
    "hep.histplot([mu1 * hist1, mu2 * hist2], bins, stack=True, histtype=\"fill\")\n",
    "hep.histplot(data, bins, histtype=\"errorbar\", color=\"black\", w2=data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "perfect-detection",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    <b>Exercise:</b> Repeat the fit, but use the same normalization factor for both samples. <i>Hint:</i> parameters in the pyhf specification are just identified by their name and the same name can occur in multiple places in the spec.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "strange-pennsylvania",
   "metadata": {},
   "source": [
    "## Uncertainties on fit parameters and the \"post-fit\" plot\n",
    "\n",
    "Often, we are also interested in the uncertainties and correlations between fit parameters. We can use `iminuit` as a fitting backend for `pyhf` to extract them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "smooth-squad",
   "metadata": {},
   "outputs": [],
   "source": [
    "pyhf.set_backend('numpy', 'minuit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "primary-track",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters, correlations = pyhf.infer.mle.fit(data, model, return_uncertainties=True, return_correlations=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "smooth-dutch",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suburban-romance",
   "metadata": {},
   "outputs": [],
   "source": [
    "correlations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "specialized-flavor",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    <b>Questions</b>: Why are the fit parameters correlated? Why is the correlation coefficient negative?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "protected-colorado",
   "metadata": {},
   "source": [
    "We can visualize the impact of these uncertainties on our fit templates using [linear error propagation](https://en.wikipedia.org/wiki/Propagation_of_uncertainty).\n",
    "\n",
    "In this simple case, let's calculate this manually:\n",
    "\n",
    "$$\\sigma_{\\lambda_b}^2 = \\left(\\frac{\\partial \\lambda_b}{\\partial \\mu_1}\\sigma_{\\mu_1}\\right)^2 + \\left(\\frac{\\partial \\lambda_b}{\\partial \\mu_2}\\sigma_{\\mu_2}\\right)^2 + 2 \\frac{\\partial \\lambda_b}{\\partial \\mu_1}\\frac{\\partial \\lambda_b}{\\partial \\mu_2}\\sigma_{\\mu_1}\\sigma_{\\mu_2}\\rho_{12} = \\left(b_1\\sigma_{\\mu_1}\\right)^2 + \\left(b_2\\sigma_{\\mu_2}\\right)^2 + 2 b_1 b_2 \\sigma_{b_1}\\sigma_{b_2}\\rho_{12}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "resistant-harvey",
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma1, sigma2 = parameters[:, 1]\n",
    "sigma1, sigma2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unknown-drain",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_error = np.sqrt(\n",
    "    (hist1 * sigma1) ** 2 + (hist2 * sigma2) ** 2 + 2 * hist1 * hist2 * sigma1 * sigma2 * correlations[0][1]\n",
    ")\n",
    "hist_error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "environmental-walker",
   "metadata": {},
   "source": [
    "For more generic templates/functions you can do that automatically. Either use the [`uncertainties`](https://pythonhosted.org/uncertainties/) package (for functions described by simple formulas) or calculate it numerically by varying each parameter up and down and using half of the resulting interval as a replacement for $\\frac{\\partial f}{\\partial x_i}\\sigma_{x_i}$.\n",
    "\n",
    "A generic function for using this method with `pyhf` models is currently provided by the [`cabinetry`](https://github.com/alexander-held/cabinetry) package from Alexander Held with the function [`cabinetry.model_utils.calculate_stdev`](https://github.com/alexander-held/cabinetry/blob/c8668005e899556675b5e646e127908849bfe597/src/cabinetry/model_utils.py#L176)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "major-indonesian",
   "metadata": {},
   "outputs": [],
   "source": [
    "def errorband(bins, hist, hist_error, ax=None):\n",
    "    ax = ax or plt.gca()\n",
    "    n = hist\n",
    "    s = hist_error\n",
    "\n",
    "    def extend(x):\n",
    "        return np.append(x, x[-1])\n",
    "\n",
    "    ax.fill_between(\n",
    "        bins,\n",
    "        extend(n - s),\n",
    "        extend(n + s),\n",
    "        step=\"post\",\n",
    "        color=\"black\",\n",
    "        alpha=0.3,\n",
    "        linewidth=0\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "digital-retail",
   "metadata": {},
   "outputs": [],
   "source": [
    "hep.histplot([mu1 * hist1, mu2 * hist2], bins, stack=True, histtype=\"fill\")\n",
    "hep.histplot(data, bins, histtype=\"errorbar\", color=\"black\", w2=data)\n",
    "errorband(bins, mu1 * hist1 + mu2 * hist2, hist_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rational-edition",
   "metadata": {},
   "source": [
    "This errorband is often referred to as \"post-fit\" uncertainties."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "theoretical-breach",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    <b>Note:</b> Since the same normalization factor is used across all bins, the uncertainty on the template depends on all data points. Therefore the interval is smaller than the expected fluctuation in each individual bin.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "improved-nothing",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    <b>Question:</b> What would happen if you fit an independent template per histogram bin? What would the uncertainty look like?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "worst-beast",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    <b>Exercise [hard]:</b> Try it out! Hint: One way to do this is by defining a sample for each bin (with an individual normalization factor for each bin) and setting all other bins to 0.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "descending-feedback",
   "metadata": {},
   "source": [
    "# Advanced:  Uncertainties on the histogram templates\n",
    "\n",
    "Our histogram templates are usually derived by MC simulations. We typically want to assign uncertainties on the templates themselves. Those can be of different origin, but mostly fall into the following categories:\n",
    "\n",
    "* **MC stat. error**: Statistical uncertainty due to the limited simulated sample size. The relative uncertainty on the expected count in a histogram bin is given by $\\sqrt{N}/N$ for a bin with $N$ simulated events or $\\sqrt{\\sum(w_i^2)}/{\\sum w_i}$ for weighted events with weights $w_i$ in that bin.\n",
    "* **Experimental uncertainties**: Uncertainties on detector simulation or reconstruction/calibration. Many of them are evaluated by re-running the full analysis chain with certain parameters varied up and down by one standard deviation of some measured/calibrated parameters.\n",
    "* **Theory uncertainties**: Uncertainties on cross sections and on the choice of different theoretical models/approximations (e.g. parton shower) or parameters. Cross section uncertainties affect the normalization, while the others are typically evaluated by re-running the simulation with parameters changed or models/algorithms replaced.\n",
    "\n",
    "In all cases we need to provide additional input to parametrize the Likelihood template. In `pyhf` this is done by specifying `modifiers` for the samples we want to assign uncertainties to. See\n",
    "\n",
    "https://pyhf.readthedocs.io/en/v0.6.1/likelihood.html#modifiers\n",
    "\n",
    "for an overview. Each modifier will come with additional parameters in the Likelihood template. We will discuss 2 cases:\n",
    "\n",
    "* Uncorrelated uncertainties per bin\n",
    "* Correlated uncertainty across all bins\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uncorrelated uncertainties per bin\n",
    "\n",
    "Uncorrelated in this sense means the uncertainty affects each bin individually. This is what we want for the **MC stat. error**.\n",
    "\n",
    "Let's assume we want to fit just one histogram  this time (using `hist1` as a template) and we have determined the (absolute) uncertainty per bin as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist1_uncorr_err = np.array([0.4 , 0.4 , 0.3 , 0.2 , 0.15, 0.4 , 0.45, 0.5 , 0.3 , 0.35])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to our normalization factor, we add another modifier to the sample of type `shapesys`. This will create a template with an additional uncertainty for each bin. We need to specify the uncertainty per bin for the `\"data\"` entry of the modifier dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = [\n",
    "    {\n",
    "        \"name\": \"sample1\",\n",
    "        \"data\": list(hist1),\n",
    "        \"modifiers\": [\n",
    "            # first modifier: normalization factor\n",
    "            {\"name\": \"mu1\", \"type\": \"normfactor\", \"data\" : None},\n",
    "            # second modifier: uncorrelated uncertainties per bin\n",
    "            {\n",
    "                \"name\": \"uncorrelated_uncertainties\",\n",
    "                \"type\": \"shapesys\",\n",
    "                \"data\" : list(hist1_uncorr_err)\n",
    "            }\n",
    "        ],\n",
    "    },\n",
    "]\n",
    "spec = {\"channels\" : [{\"name\" : \"singlechannel\", \"samples\" : samples}]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_uncorr = pyhf.Model(spec, poi_name=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will introduce one additional fit parameter per bin, so we have 11 parameters in total, 1 for the normalization factor and 10 for the uncorrelated uncertainty per bin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_uncorr.config.npars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does this work? Well, as mentioned these uncertainties usually come from the limited amount of MC samples. In some sense, this is an additional measurement with data from our MC simulation! So we build this into our model by adding **auxiliary data** that emulates this. The pdf for building the likelihood function becomes\n",
    "\n",
    "$$p(\\vec n, \\vec a|\\vec \\lambda, \\vec \\gamma) = \\prod_{\\mathrm{data\\,bin}\\, i} \\mathrm{Pois}(n_i | \\lambda_i) \\prod_{\\mathrm{aux\\,data\\, bin}\\, i} \\mathrm{Pois}(a_i | \\gamma_i a_i)$$\n",
    "\n",
    "where in our example\n",
    "\n",
    "$$\\lambda_i = \\mu_1 \\gamma_i b_i$$\n",
    "\n",
    "We determine the amount of auxiliary data $a_i$ per bin by asking \"which effective number of entries would give us that relative uncertainty\". That is given by\n",
    "\n",
    "$$\\frac{\\sqrt{a_i}}{a_i} = \\frac{\\Delta b_i}{b_i} \\rightarrow a_i = \\left(\\frac{b_i}{\\Delta b_i}\\right)^2$$\n",
    "\n",
    "where $b$ is the expected count and $\\Delta b$ the absolute uncertainty in the corresponding histogram bin.\n",
    "\n",
    "So the auxiliary data for each bin is given by:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(hist1 / hist1_uncorr_err) ** 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which is also what `pyhf` calculated for us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_uncorr.config.auxdata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The auxdata is set to match the expected counts exactly for the intial parameter values $\\gamma_i = 1$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_parameters = model_uncorr.config.suggested_init()\n",
    "hep.histplot(model_uncorr.expected_auxdata(initial_parameters), bins)\n",
    "hep.histplot(model_uncorr.config.auxdata, bins, histtype=\"errorbar\")\n",
    "print(initial_parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the parameters $\\gamma_i$ need to simultaneously fit the real and auxiliary data. They are therefore not completely free. We say they are \"constrained\".\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "    <b>Exercise</b> Try to manually tune the parameters in the following interactive plot. Observe how the templates that need to fit real and auxiliary data change. Also have a look at the value of the negative log likelihood (which is the objective we want to minimize in the fit). Why can't we get a perfect fit to the real data?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You don't need to understand right now what is happening in this code cell, first focus on the application\n",
    "\n",
    "sliders_gamma = {\n",
    "    f\"gamma{i}\" : widgets.FloatSlider(\n",
    "        1.0,\n",
    "        min=0.1,\n",
    "        max=2.0,\n",
    "        orientation=\"vertical\",\n",
    "        continuous_update=False,\n",
    "        description=f\"Î³{i}\",\n",
    "        layout=widgets.Layout(width='35px')\n",
    "    )\n",
    "    for i in range(1, 11)\n",
    "}\n",
    "\n",
    "slider_mu = widgets.FloatSlider(\n",
    "    1.0, min=0.1, max=10.0, description=\"Norm factor\", continuous_update=False\n",
    ")\n",
    "\n",
    "def plot(mu, **kwargs):\n",
    "    fig, axs = plt.subplots(ncols=2, figsize=(20, 5))\n",
    "\n",
    "    parameters = model_uncorr.config.suggested_init()\n",
    "    \n",
    "    parameters[0] = mu\n",
    "    \n",
    "    for k in kwargs:\n",
    "        i = int(k.replace(\"gamma\", \"\"))\n",
    "        parameters[i] = kwargs[k]\n",
    "\n",
    "    hep.histplot(model_uncorr.expected_actualdata(parameters), bins, ax=axs[0])\n",
    "    hep.histplot(data, bins, yerr=np.sqrt(data), histtype=\"errorbar\", color=\"black\", ax=axs[0])\n",
    "    hep.histplot(model_uncorr.expected_auxdata(parameters), bins, ax=axs[1])\n",
    "    hep.histplot(\n",
    "        model_uncorr.config.auxdata,\n",
    "        bins,\n",
    "        yerr=np.sqrt(model_uncorr.config.auxdata),\n",
    "        histtype=\"errorbar\", color=\"red\",\n",
    "        ax=axs[1]\n",
    "    )\n",
    "    axs[0].set_title(\"Actual data\")\n",
    "    axs[1].set_title(\"Auxiliary data\")\n",
    "    \n",
    "    print(\n",
    "        \"Negative Log-Likelihood: \"\n",
    "        f\"{- model_uncorr.logpdf(parameters, np.concatenate([data, model_uncorr.config.auxdata]))[0]:.3f}\"\n",
    "    )\n",
    "    \n",
    "interactive_plot = widgets.interactive_output(plot, dict(sliders_gamma, mu=slider_mu))\n",
    "interactive_plot.layout.height = \"350px\"\n",
    "\n",
    "def fit(b):\n",
    "    parameters = pyhf.infer.mle.fit(np.concatenate([data, model_uncorr.config.auxdata]), model_uncorr)\n",
    "    slider_mu.value = parameters[0]\n",
    "    for k in sliders_gamma:\n",
    "        i = int(k.replace(\"gamma\", \"\"))\n",
    "        sliders_gamma[k].value = parameters[i]\n",
    "        \n",
    "button = widgets.Button(description=\"Fit\")\n",
    "button.on_click(fit)\n",
    "\n",
    "display(\n",
    "    slider_mu,\n",
    "    widgets.HBox([sliders_gamma[f\"gamma{i}\"] for i in range(1, 11)]),\n",
    "    button,\n",
    "    interactive_plot\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
